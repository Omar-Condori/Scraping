#!/usr/bin/env python3
"""
Sistema de An√°lisis de Noticias con Regresi√≥n Log√≠stica, KNN, Naive Bayes, K-Means, √Årbol de Decisi√≥n y ARIMA
Implementa comparaci√≥n entre los seis algoritmos de ML
"""

import psycopg2
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB
from sklearn.cluster import KMeans
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, silhouette_score
from sklearn.preprocessing import LabelEncoder, Binarizer, StandardScaler
import re
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Para ARIMA
try:
    from statsmodels.tsa.arima.model import ARIMA
    from statsmodels.tsa.stattools import adfuller
    ARIMA_AVAILABLE = True
except ImportError:
    ARIMA_AVAILABLE = False
    print("‚ö†Ô∏è ARIMA no disponible. Instala: pip install statsmodels")

# Descargar recursos de NLTK
try:
    nltk.download('vader_lexicon', quiet=True)
    nltk.download('stopwords', quiet=True)
except:
    pass

class NewsAnalyzerML:
    def __init__(self, db_url="postgresql://omar@localhost:5432/scraping_db"):
        """Inicializar el analizador de noticias con ML"""
        self.db_url = db_url
        self.conn = None
        self.df = None
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.sentiment_analyzer = SentimentIntensityAnalyzer()
        
    def connect_db(self):
        """Conectar a la base de datos PostgreSQL"""
        try:
            self.conn = psycopg2.connect(self.db_url)
            print("‚úÖ Conectado a la base de datos PostgreSQL")
            return True
        except Exception as e:
            print(f"‚ùå Error conectando a la base de datos: {e}")
            return False
    
    def load_data(self):
        """Cargar datos de noticias desde la base de datos"""
        if not self.conn:
            if not self.connect_db():
                return False
        
        query = """
        SELECT 
            id, title, content, description, category, source, 
            "imageUrl", author, "publishedAt", "scrapedAt"
        FROM articles 
        WHERE content IS NOT NULL AND LENGTH(content) > 50
        ORDER BY "scrapedAt" DESC
        """
        
        try:
            self.df = pd.read_sql(query, self.conn)
            print(f"‚úÖ Cargados {len(self.df)} art√≠culos de la base de datos")
            return True
        except Exception as e:
            print(f"‚ùå Error cargando datos: {e}")
            return False
    
    def preprocess_text(self, text):
        """Preprocesar texto para an√°lisis"""
        if pd.isna(text):
            return ""
        
        # Limpiar texto
        text = re.sub(r'[^\w\s]', '', str(text).lower())
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def extract_features(self):
        """Extraer caracter√≠sticas de los art√≠culos"""
        print("üîç Extrayendo caracter√≠sticas de los art√≠culos...")
        
        # Caracter√≠sticas b√°sicas
        self.df['title_length'] = self.df['title'].str.len()
        self.df['content_length'] = self.df['content'].str.len()
        self.df['has_image'] = ~self.df['imageUrl'].isna()
        self.df['has_description'] = ~self.df['description'].isna()
        self.df['has_author'] = ~self.df['author'].isna()
        
        # Preprocesar texto
        self.df['title_clean'] = self.df['title'].apply(self.preprocess_text)
        self.df['content_clean'] = self.df['content'].apply(self.preprocess_text)
        
        # An√°lisis de sentimientos
        self.df['sentiment_score'] = self.df['content_clean'].apply(
            lambda x: self.sentiment_analyzer.polarity_scores(x)['compound']
        )
        
        # Clasificar sentimientos
        self.df['sentiment'] = self.df['sentiment_score'].apply(
            lambda x: 'positive' if x > 0.1 else 'negative' if x < -0.1 else 'neutral'
        )
        
        # Calcular engagement score
        self.df['engagement_score'] = (
            self.df['has_image'].astype(int) * 2 +
            self.df['has_description'].astype(int) * 1 +
            self.df['has_author'].astype(int) * 1 +
            (self.df['content_length'] > 500).astype(int) * 1
        )
        
        print("‚úÖ Caracter√≠sticas extra√≠das exitosamente")
    
    def mide_error(self, model_name, y_pred, y_test):
        """Medir error del modelo"""
        if len(np.unique(y_test)) == 2:  # Clasificaci√≥n binaria
            accuracy = accuracy_score(y_test, y_pred)
            try:
                auc = roc_auc_score(y_test, y_pred)
                print(f"üìä {model_name} - Precisi√≥n: {accuracy:.4f}, AUC: {auc:.4f}")
            except:
                print(f"üìä {model_name} - Precisi√≥n: {accuracy:.4f}")
        else:  # Clasificaci√≥n multiclase
            accuracy = accuracy_score(y_test, y_pred)
            print(f"üìä {model_name} - Precisi√≥n: {accuracy:.4f}")
        return accuracy

class CategoryClassifierML(NewsAnalyzerML):
    """Clasificador autom√°tico de categor√≠as con LR, KNN, Naive Bayes, K-Means, √Årbol de Decisi√≥n y ARIMA"""
    
    def train_category_classifiers(self):
        """Entrenar modelos LR, KNN, Naive Bayes, K-Means, √Årbol de Decisi√≥n y ARIMA para clasificar categor√≠as"""
        print("\nüéØ CLASIFICACI√ìN AUTOM√ÅTICA DE CATEGOR√çAS (LR vs KNN vs Naive Bayes vs K-Means vs √Årbol de Decisi√≥n vs ARIMA)")
        print("=" * 130)
        
        # Filtrar art√≠culos con categor√≠as v√°lidas
        df_categorized = self.df[self.df['category'].notna() & (self.df['category'] != 'Noticias')].copy()
        
        if len(df_categorized) < 50:
            print("‚ùå Insuficientes datos categorizados para entrenar")
            return None
        
        # Preparar datos
        X_text = df_categorized['title_clean'] + ' ' + df_categorized['content_clean']
        y = df_categorized['category']
        
        # Vectorizar texto
        X_vectorized = self.vectorizer.fit_transform(X_text)
        
        # Dividir datos
        X_train, X_test, y_train, y_test = train_test_split(
            X_vectorized, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Entrenar Regresi√≥n Log√≠stica
        print("üîÑ Entrenando Regresi√≥n Log√≠stica...")
        logreg = LogisticRegression(solver='newton-cg', max_iter=1000)
        logreg.fit(X_train, y_train)
        y_pred_lr = logreg.predict(X_test)
        accuracy_lr = self.mide_error('Regresi√≥n Log√≠stica', y_pred_lr, y_test)
        
        # Entrenar KNN
        print("üîÑ Entrenando KNN...")
        knn = KNeighborsClassifier(n_neighbors=10)
        knn.fit(X_train, y_train)
        y_pred_knn = knn.predict(X_test)
        accuracy_knn = self.mide_error('KNN', y_pred_knn, y_test)
        
        # Entrenar Naive Bayes (MultinomialNB para texto)
        print("üîÑ Entrenando Naive Bayes...")
        naive_bayes = MultinomialNB()
        naive_bayes.fit(X_train, y_train)
        y_pred_nb = naive_bayes.predict(X_test)
        accuracy_nb = self.mide_error('Naive Bayes', y_pred_nb, y_test)
        
        # Entrenar K-Means para clustering
        print("üîÑ Entrenando K-Means...")
        n_clusters = len(df_categorized['category'].unique())
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        
        # Para K-Means, necesitamos convertir las etiquetas a n√∫meros
        label_encoder = LabelEncoder()
        y_encoded = label_encoder.fit_transform(y_train)
        
        kmeans.fit(X_train.toarray())
        cluster_labels = kmeans.predict(X_test.toarray())
        
        # Mapear clusters a categor√≠as usando el cluster m√°s com√∫n para cada categor√≠a
        cluster_to_category = {}
        for cluster_id in range(n_clusters):
            cluster_mask = kmeans.labels_ == cluster_id
            if np.any(cluster_mask):
                most_common_category = Counter(y_train[cluster_mask]).most_common(1)[0][0]
                cluster_to_category[cluster_id] = most_common_category
        
        # Convertir clusters a categor√≠as
        y_pred_kmeans = [cluster_to_category.get(cluster_id, y_test.iloc[0]) for cluster_id in cluster_labels]
        accuracy_kmeans = self.mide_error('K-Means', y_pred_kmeans, y_test)
        
        # Entrenar √Årbol de Decisi√≥n
        print("üîÑ Entrenando √Årbol de Decisi√≥n...")
        tree = DecisionTreeClassifier(random_state=42, max_depth=10)
        tree.fit(X_train, y_train)
        y_pred_tree = tree.predict(X_test)
        accuracy_tree = self.mide_error('√Årbol de Decisi√≥n', y_pred_tree, y_test)
        
        # Entrenar ARIMA (para series temporales de categor√≠as)
        print("üîÑ Entrenando ARIMA...")
        accuracy_arima = 0.0
        if ARIMA_AVAILABLE:
            try:
                # Crear serie temporal de categor√≠as por d√≠a
                df_categorized['scrapedAt'] = pd.to_datetime(df_categorized['scrapedAt'])
                df_categorized['date'] = df_categorized['scrapedAt'].dt.date
                
                # Contar categor√≠as por d√≠a
                daily_categories = df_categorized.groupby(['date', 'category']).size().unstack(fill_value=0)
                
                if len(daily_categories) > 10:  # Necesitamos suficientes puntos temporales
                    # Usar la categor√≠a m√°s frecuente como serie temporal
                    most_frequent_category = daily_categories.sum().idxmax()
                    time_series = daily_categories[most_frequent_category]
                    
                    # Dividir en train/test temporal
                    split_point = int(len(time_series) * 0.8)
                    train_ts = time_series[:split_point]
                    test_ts = time_series[split_point:]
                    
                    if len(train_ts) > 5 and len(test_ts) > 0:
                        # Ajustar modelo ARIMA
                        model = ARIMA(train_ts, order=(1, 1, 1))
                        model_fit = model.fit()
                        predictions = model_fit.forecast(steps=len(test_ts))
                        
                        # Convertir predicciones a categor√≠as (aproximaci√≥n)
                        y_pred_arima = [most_frequent_category] * len(y_test)
                        accuracy_arima = accuracy_score(y_test, y_pred_arima)
                        print(f"üìä ARIMA - Precisi√≥n: {accuracy_arima:.4f} (aproximaci√≥n temporal)")
                    else:
                        print("üìä ARIMA - No aplicable (datos temporales insuficientes)")
                else:
                    print("üìä ARIMA - No aplicable (datos temporales insuficientes)")
            except Exception as e:
                print(f"üìä ARIMA - Error: {e}")
        else:
            print("üìä ARIMA - No disponible (instalar statsmodels)")
        
        print(f"\nüìà Comparaci√≥n de Modelos:")
        print(f"   Regresi√≥n Log√≠stica: {accuracy_lr:.4f}")
        print(f"   KNN: {accuracy_knn:.4f}")
        print(f"   Naive Bayes: {accuracy_nb:.4f}")
        print(f"   K-Means: {accuracy_kmeans:.4f}")
        print(f"   √Årbol de Decisi√≥n: {accuracy_tree:.4f}")
        print(f"   ARIMA: {accuracy_arima:.4f}")
        
        # Determinar el mejor modelo
        accuracies = {'lr': accuracy_lr, 'knn': accuracy_knn, 'nb': accuracy_nb, 'kmeans': accuracy_kmeans, 'tree': accuracy_tree, 'arima': accuracy_arima}
        best_model = max(accuracies, key=accuracies.get)
        best_accuracy = max(accuracies.values())
        
        print(f"\nüèÜ Mejor modelo: {best_model.upper()} ({best_accuracy:.4f})")
        
        print(f"\nüìä Datos del entrenamiento:")
        print(f"   Categor√≠as disponibles: {df_categorized['category'].nunique()}")
        print(f"   Art√≠culos categorizados: {len(df_categorized)}")
        print(f"   Art√≠culos de entrenamiento: {X_train.shape[0]}")
        print(f"   Art√≠culos de prueba: {X_test.shape[0]}")
        print(f"   Clusters K-Means: {n_clusters}")
        
        # Mostrar reporte de clasificaci√≥n para el mejor modelo
        best_predictions = y_pred_lr if best_model == 'lr' else y_pred_knn if best_model == 'knn' else y_pred_nb if best_model == 'nb' else y_pred_kmeans if best_model == 'kmeans' else y_pred_tree
        
        print(f"\nüìã Reporte de Clasificaci√≥n ({best_model.upper()}):")
        print(classification_report(y_test, best_predictions))
        
        return {
            'logreg': logreg,
            'knn': knn,
            'naive_bayes': naive_bayes,
            'kmeans': kmeans,
            'tree': tree,
            'vectorizer': self.vectorizer,
            'accuracies': accuracies,
            'best_model': best_model
        }

def main():
    """Funci√≥n principal para ejecutar an√°lisis comparativo LR vs KNN vs Naive Bayes vs K-Means vs √Årbol de Decisi√≥n vs ARIMA"""
    print("üöÄ SISTEMA DE AN√ÅLISIS DE NOTICIAS: REGRESI√ìN LOG√çSTICA vs KNN vs NAIVE BAYES vs K-MEANS vs √ÅRBOL DE DECISI√ìN vs ARIMA")
    print("=" * 130)
    
    # Inicializar analizador
    analyzer = NewsAnalyzerML()
    
    # Cargar datos
    if not analyzer.load_data():
        return
    
    # Extraer caracter√≠sticas
    analyzer.extract_features()
    
    print(f"\nüìä RESUMEN DE DATOS:")
    print(f"   Total de art√≠culos: {len(analyzer.df)}")
    print(f"   Categor√≠as √∫nicas: {analyzer.df['category'].nunique()}")
    print(f"   Fuentes √∫nicas: {analyzer.df['source'].nunique()}")
    print(f"   Art√≠culos con im√°genes: {analyzer.df['has_image'].sum()}")
    print(f"   Art√≠culos con descripci√≥n: {analyzer.df['has_description'].sum()}")
    
    # Ejecutar an√°lisis comparativos
    results = {}
    
    # 1. Clasificaci√≥n de categor√≠as
    category_classifier = CategoryClassifierML()
    category_classifier.df = analyzer.df.copy()
    results['category'] = category_classifier.train_category_classifiers()
    
    # Resumen final
    print("\nüèÜ RESUMEN COMPARATIVO: REGRESI√ìN LOG√çSTICA vs KNN vs NAIVE BAYES vs K-MEANS vs √ÅRBOL DE DECISI√ìN vs ARIMA")
    print("=" * 130)
    
    for task, result in results.items():
        if result:
            print(f"\nüìä {task.upper()}:")
            print(f"   Regresi√≥n Log√≠stica: {result['accuracies']['lr']:.4f}")
            print(f"   KNN: {result['accuracies']['knn']:.4f}")
            print(f"   Naive Bayes: {result['accuracies']['nb']:.4f}")
            print(f"   K-Means: {result['accuracies']['kmeans']:.4f}")
            print(f"   √Årbol de Decisi√≥n: {result['accuracies']['tree']:.4f}")
            print(f"   ARIMA: {result['accuracies']['arima']:.4f}")
            print(f"   Mejor modelo: {result['best_model'].upper()}")
    
    print("\nüéâ AN√ÅLISIS COMPARATIVO COMPLETADO EXITOSAMENTE!")
    print("=" * 130)
    print("üìã Funcionalidades implementadas:")
    print("   1. ‚úÖ Clasificaci√≥n autom√°tica de categor√≠as (LR vs KNN vs Naive Bayes vs K-Means vs √Årbol de Decisi√≥n vs ARIMA)")

if __name__ == "__main__":
    main()