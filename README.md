# üì∞ Sistema de Scraping de Noticias - Next.js

Un sistema completo de scraping de noticias peruanas desarrollado con Next.js, que permite extraer, almacenar y visualizar noticias de m√∫ltiples fuentes con im√°genes y descripciones.

## üöÄ Caracter√≠sticas Principales

- **Scraping Din√°mico**: Agrega y gestiona fuentes de noticias desde la interfaz web
- **Categorizaci√≥n Inteligente**: Sistema de categor√≠as personalizable
- **Im√°genes y Descripciones**: Extracci√≥n autom√°tica de contenido multimedia
- **Interfaz Moderna**: Dashboard responsive con TailwindCSS
- **Base de Datos**: PostgreSQL con Prisma ORM
- **Automatizaci√≥n**: Cron jobs para actualizaciones autom√°ticas
- **API REST**: Endpoints para gesti√≥n de datos

## üõ†Ô∏è Tecnolog√≠as Utilizadas

### **Frontend**
- **Next.js 14.0.4**: Framework React full-stack
- **React 18**: Biblioteca de interfaz de usuario
- **TypeScript**: Lenguaje de programaci√≥n con tipado est√°tico
- **TailwindCSS 3.3.0**: Framework CSS utility-first
- **Lucide React**: Librer√≠a de iconos

### **Backend**
- **Node.js**: Runtime de JavaScript
- **Next.js API Routes**: Endpoints del servidor
- **Prisma 5.7.1**: ORM para base de datos
- **PostgreSQL**: Base de datos relacional

### **Scraping y Automatizaci√≥n**
- **Cheerio 1.0.0-rc.12**: Parser HTML del lado del servidor
- **Axios 1.6.2**: Cliente HTTP para requests
- **node-cron 3.0.3**: Programador de tareas (cron jobs)

### **Utilidades**
- **date-fns 2.30.0**: Manipulaci√≥n de fechas
- **tsx 4.20.5**: Ejecutor de TypeScript
- **PostCSS**: Procesador CSS
- **ESLint**: Linter de c√≥digo

### **Despliegue y Desarrollo**
- **Docker**: Containerizaci√≥n
- **Vercel**: Plataforma de despliegue
- **Git/GitHub**: Control de versiones

## üìÅ Estructura del Proyecto

```
Scraping/
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îú‚îÄ‚îÄ db.ts                    # Configuraci√≥n de Prisma
‚îÇ   ‚îú‚îÄ‚îÄ cron.ts                  # Configuraci√≥n de cron jobs
‚îÇ   ‚îî‚îÄ‚îÄ scraping/
‚îÇ       ‚îú‚îÄ‚îÄ index.ts             # Scraping est√°tico
‚îÇ       ‚îú‚îÄ‚îÄ hackerNews.ts        # Scraper de Hacker News
‚îÇ       ‚îî‚îÄ‚îÄ dynamicScraper.ts    # Scraper din√°mico
‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îú‚îÄ‚îÄ api/                     # API Routes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ articles/            # Endpoints de art√≠culos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sources/             # Gesti√≥n de fuentes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ categories/          # Gesti√≥n de categor√≠as
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scrape/              # Scraping manual
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stats/               # Estad√≠sticas
‚îÇ   ‚îú‚îÄ‚îÄ index.tsx                # Dashboard principal
‚îÇ   ‚îú‚îÄ‚îÄ articles/                # P√°gina de art√≠culos
‚îÇ   ‚îú‚îÄ‚îÄ dashboard/               # Estad√≠sticas detalladas
‚îÇ   ‚îî‚îÄ‚îÄ admin/                   # Panel de administraci√≥n
‚îú‚îÄ‚îÄ prisma/
‚îÇ   ‚îî‚îÄ‚îÄ schema.prisma            # Esquema de base de datos
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ initCategories.ts        # Inicializaci√≥n de categor√≠as
‚îú‚îÄ‚îÄ styles/
‚îÇ   ‚îî‚îÄ‚îÄ globals.css              # Estilos globales
‚îî‚îÄ‚îÄ package.json                 # Dependencias y scripts
```

## üóÑÔ∏è Base de Datos

### **Modelos Principales**

#### **Article**
```prisma
model Article {
  id          String   @id @default(cuid())
  title       String
  url         String   @unique
  content     String?
  description String?  // Descripci√≥n corta
  imageUrl    String?  // URL de imagen
  author      String?
  source      String
  category    String?
  publishedAt DateTime?
  scrapedAt   DateTime @default(now())
  createdAt   DateTime @default(now())
  updatedAt   DateTime @updatedAt
}
```

#### **ScrapingSource**
```prisma
model ScrapingSource {
  id          String   @id @default(cuid())
  name        String
  url         String
  category    String
  selector    String?  // Selectores CSS personalizados
  maxItems    Int      @default(10)
  isActive    Boolean  @default(true)
  createdAt   DateTime @default(now())
  updatedAt   DateTime @updatedAt
}
```

#### **Category**
```prisma
model Category {
  id          String   @id @default(cuid())
  name        String   @unique
  description String?
  color       String?
  createdAt   DateTime @default(now())
  updatedAt   DateTime @updatedAt
}
```

## üöÄ Instalaci√≥n y Configuraci√≥n

### **1. Prerrequisitos**
- Node.js 18+ 
- PostgreSQL 12+
- Git

### **2. Clonar el Repositorio**
```bash
git clone https://github.com/Omar-Condori/Scraping.git
cd Scraping
```

### **3. Instalar Dependencias**
```bash
npm install
```

### **4. Configurar Base de Datos**
```bash
# Crear base de datos PostgreSQL
createdb scraping_db

# Configurar variables de entorno
cp env.example .env.local
# Editar .env.local con tu configuraci√≥n de PostgreSQL
```

### **5. Configurar Prisma**
```bash
# Generar cliente de Prisma
npm run db:generate

# Aplicar migraciones
npm run db:push

# Inicializar categor√≠as por defecto
npm run init:categories
```

### **6. Ejecutar la Aplicaci√≥n**
```bash
# Modo desarrollo
npm run dev

# La aplicaci√≥n estar√° disponible en http://localhost:3000
```

## üìñ Manual de Usuario

### **üè† Dashboard Principal**

**URL**: `http://localhost:3000`

**Funcionalidades**:
- **Resumen de estad√≠sticas**: Total de art√≠culos, fuentes activas, categor√≠as
- **Actividad reciente**: √öltimos scrapings realizados
- **Navegaci√≥n r√°pida**: Enlaces a art√≠culos, dashboard y administraci√≥n

### **üì∞ P√°gina de Art√≠culos**

**URL**: `http://localhost:3000/articles`

**Funcionalidades**:
- **Lista de art√≠culos**: Muestra 5 art√≠culos por p√°gina
- **Im√°genes**: Cada art√≠culo muestra su imagen principal
- **Descripciones**: Resumen de cada noticia
- **Filtros**:
  - **Categor√≠a**: Pol√≠tica, Sociedad, Internacional, etc.
  - **Fuente**: RPP, Per√∫ 21, El Comercio, La Rep√∫blica
- **B√∫squeda**: Buscar por t√≠tulo o contenido
- **Paginaci√≥n**: Navegar entre p√°ginas

**C√≥mo usar**:
1. Selecciona una categor√≠a del filtro
2. Elige una fuente espec√≠fica
3. Usa la barra de b√∫squeda para encontrar art√≠culos
4. Haz clic en "Ver art√≠culo" para leer la noticia completa

### **üìä Dashboard de Estad√≠sticas**

**URL**: `http://localhost:3000/dashboard`

**Funcionalidades**:
- **Estad√≠sticas generales**: Total de art√≠culos, fuentes, categor√≠as
- **Distribuci√≥n por categor√≠a**: Gr√°fico de barras
- **Distribuci√≥n por fuente**: Gr√°fico circular
- **Actividad reciente**: Log de scrapings
- **Art√≠culos por d√≠a**: Gr√°fico de l√≠neas

### **‚öôÔ∏è Panel de Administraci√≥n**

**URL**: `http://localhost:3000/admin`

**Funcionalidades**:

#### **Gesti√≥n de Fuentes**
- **Agregar fuente**: 
  - Nombre de la fuente
  - URL del sitio web
  - Categor√≠a asignada
  - N√∫mero m√°ximo de art√≠culos
- **Editar fuente**: Modificar configuraci√≥n existente
- **Activar/Desactivar**: Toggle para controlar el scraping

#### **Gesti√≥n de Categor√≠as**
- **Agregar categor√≠a**:
  - Nombre de la categor√≠a
  - Descripci√≥n
  - Color personalizado
- **Editar categor√≠a**: Modificar informaci√≥n existente

**C√≥mo usar**:
1. Ve a la secci√≥n "Gesti√≥n de Fuentes"
2. Completa el formulario con la URL del sitio
3. Selecciona la categor√≠a apropiada
4. Guarda la fuente
5. La fuente aparecer√° en los filtros de art√≠culos

## üîß Comandos √ötiles

### **Desarrollo**
```bash
# Ejecutar en modo desarrollo
npm run dev

# Construir para producci√≥n
npm run build

# Ejecutar en producci√≥n
npm start
```

### **Base de Datos**
```bash
# Generar cliente de Prisma
npm run db:generate

# Aplicar cambios al esquema
npm run db:push

# Crear migraci√≥n
npm run db:migrate

# Abrir Prisma Studio
npm run db:studio
```

### **Scraping**
```bash
# Scraping manual (est√°tico)
curl -X POST http://localhost:3000/api/scrape

# Scraping din√°mico
curl -X POST http://localhost:3000/api/scrape-dynamic

# Inicializar categor√≠as
npm run init:categories
```

### **Base de Datos PostgreSQL**
```bash
# Conectar a la base de datos
psql -d scraping_db

# Ver todas las tablas
\dt

# Contar art√≠culos por categor√≠a
SELECT category, COUNT(*) FROM articles GROUP BY category;

# Ver art√≠culos con im√°genes
SELECT title, "imageUrl" FROM articles WHERE "imageUrl" IS NOT NULL LIMIT 5;

# Ver art√≠culos con descripciones
SELECT title, description FROM articles WHERE description IS NOT NULL LIMIT 5;
```

## üîÑ Automatizaci√≥n

### **Cron Jobs Configurados**

El sistema incluye tareas autom√°ticas:

- **Cada hora**: Scraping din√°mico de todas las fuentes activas
- **Cada 6 horas**: Scraping completo del sistema
- **Logs autom√°ticos**: Registro de todas las operaciones

### **Configuraci√≥n de Cron**
```typescript
// lib/cron.ts
cron.schedule('0 * * * *', () => {
  console.log('Ejecutando scraping cada hora...')
  scrapeAllDynamicSources()
})

cron.schedule('0 */6 * * *', () => {
  console.log('Ejecutando scraping cada 6 horas...')
  scrapeAll()
})
```

## üåê API Endpoints

### **Art√≠culos**
- `GET /api/articles` - Listar art√≠culos con filtros
- `GET /api/articles/[id]` - Obtener art√≠culo espec√≠fico

### **Fuentes**
- `GET /api/sources` - Listar fuentes de scraping
- `POST /api/sources` - Crear nueva fuente
- `PUT /api/sources/[id]` - Actualizar fuente
- `DELETE /api/sources/[id]` - Eliminar fuente

### **Categor√≠as**
- `GET /api/categories` - Listar categor√≠as
- `POST /api/categories` - Crear nueva categor√≠a
- `PUT /api/categories/[id]` - Actualizar categor√≠a

### **Scraping**
- `POST /api/scrape` - Ejecutar scraping est√°tico
- `POST /api/scrape-dynamic` - Ejecutar scraping din√°mico

### **Estad√≠sticas**
- `GET /api/stats` - Obtener estad√≠sticas generales

## üê≥ Docker

### **Ejecutar con Docker Compose**
```bash
# Construir y ejecutar
docker-compose up --build

# Ejecutar en segundo plano
docker-compose up -d

# Ver logs
docker-compose logs -f
```

### **Archivos Docker**
- `Dockerfile`: Configuraci√≥n del contenedor de la aplicaci√≥n
- `docker-compose.yml`: Orquestaci√≥n de servicios (app + PostgreSQL)

## üöÄ Despliegue en Vercel

### **Configuraci√≥n**
1. Conecta tu repositorio GitHub a Vercel
2. Configura las variables de entorno:
   - `DATABASE_URL`: URL de tu base de datos PostgreSQL
3. Despliega autom√°ticamente

### **Variables de Entorno Requeridas**
```env
DATABASE_URL="postgresql://usuario:password@host:puerto/database"
```

## üìä Estad√≠sticas del Proyecto

### **Fuentes de Noticias Activas**
- **RPP**: Noticias generales y pol√≠ticas
- **Per√∫ 21**: Noticias internacionales
- **El Comercio**: Noticias pol√≠ticas y econ√≥micas
- **La Rep√∫blica**: Noticias de sociedad y actualidad

### **Categor√≠as Disponibles**
1. **Pol√≠tica**: Nacional, Gobierno, Congreso, Elecciones
2. **Econom√≠a / Negocios**: Finanzas, Mercados, Empleo
3. **Sociedad / Actualidad**: Ciudad, Comunidad, Sucesos
4. **Internacional**: Mundo, Latinoam√©rica
5. **Tecnolog√≠a / Ciencia**: Innovaci√≥n, Medio ambiente, Salud
6. **Cultura / Arte**: M√∫sica, Cine, Literatura
7. **Opini√≥n**: Editoriales, Columnistas
8. **Estilo de vida / Tendencias**: Gastronom√≠a, Viajes, Moda
9. **Clima y Medio ambiente**: Clima, Sostenibilidad

### **Rendimiento Actual**
- **Total de art√≠culos**: ~115
- **Art√≠culos con im√°genes**: ~52 (45%)
- **Art√≠culos con descripciones**: ~94 (82%)
- **Fuentes activas**: 4
- **Categor√≠as**: 9

## üîß Soluci√≥n de Problemas

### **Error de Conexi√≥n a Base de Datos**
```bash
# Verificar que PostgreSQL est√© ejecut√°ndose
brew services start postgresql

# Verificar conexi√≥n
psql -d scraping_db -c "SELECT 1;"
```

### **Error de Prisma**
```bash
# Regenerar cliente
npm run db:generate

# Aplicar esquema
npm run db:push
```

### **Error de Scraping**
```bash
# Verificar logs
curl -X POST http://localhost:3000/api/scrape-dynamic

# Verificar fuentes activas
psql -d scraping_db -c "SELECT name, url, \"isActive\" FROM scraping_sources;"
```

## üìù Notas de Desarrollo

### **Selectores CSS Personalizados**
El sistema permite configurar selectores CSS espec√≠ficos para cada fuente:

```json
{
  "title": ".story-item__title, h2, h3",
  "link": "a[href*='/noticias/']",
  "content": ".story-item__summary, p",
  "image": ".story-item__image img, img[src*='rpp']"
}
```

### **L√≠mites de Scraping**
- **M√°ximo por fuente**: 10 art√≠culos (configurable)
- **Timeout de requests**: 30 segundos
- **Intervalo entre scrapings**: 1 hora

### **Optimizaciones Implementadas**
- **Upsert en base de datos**: Evita duplicados
- **Lazy loading de im√°genes**: Mejora rendimiento
- **Paginaci√≥n**: Limita carga de datos
- **Cach√© de respuestas**: Reduce requests innecesarios

## ü§ù Contribuci√≥n

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver el archivo `LICENSE` para m√°s detalles.

## üë®‚Äçüíª Autor

**Omar Condori**
- GitHub: [@Omar-Condori](https://github.com/Omar-Condori)
- Proyecto: [Scraping](https://github.com/Omar-Condori/Scraping)

---

## üéØ Pr√≥ximas Mejoras

- [ ] **Sistema de notificaciones** para nuevos art√≠culos
- [ ] **Exportaci√≥n de datos** en CSV/JSON
- [ ] **An√°lisis de sentimientos** de las noticias
- [ ] **API de b√∫squeda avanzada** con filtros complejos
- [ ] **Sistema de usuarios** y autenticaci√≥n
- [ ] **Dashboard de m√©tricas** en tiempo real
- [ ] **Integraci√≥n con redes sociales** para compartir
- [ ] **Sistema de alertas** por palabras clave

---

**¬°Disfruta explorando las noticias peruanas con tu sistema de scraping personalizado!** üéâüì∞