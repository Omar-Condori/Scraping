# ğŸš€ Next.js Scraping Dashboard

Un dashboard completo de scraping construido con Next.js, que recolecta datos de mÃºltiples fuentes y los presenta en una interfaz moderna y responsiva.

## âœ¨ CaracterÃ­sticas

- **Scraping AutomÃ¡tico**: RecolecciÃ³n de datos de fuentes de noticias peruanas
- **Base de Datos**: PostgreSQL con Prisma ORM
- **Cron Jobs**: ActualizaciÃ³n automÃ¡tica cada hora
- **Frontend Moderno**: Interfaz con TailwindCSS y componentes reutilizables
- **API RESTful**: Endpoints para consultar datos
- **Dashboard**: EstadÃ­sticas y anÃ¡lisis de datos
- **AdministraciÃ³n**: Panel para gestionar fuentes y categorÃ­as
- **Deployment**: Preparado para Vercel y Docker

## ğŸ› ï¸ TecnologÃ­as

- **Frontend**: Next.js 14, React, TypeScript, TailwindCSS
- **Backend**: Next.js API Routes, Prisma ORM
- **Base de Datos**: PostgreSQL
- **Scraping**: Cheerio, Axios
- **Scheduling**: node-cron
- **Deployment**: Vercel, Docker

## ğŸ“Š Fuentes Actuales

- **RPP**: Noticias de polÃ­tica
- **PerÃº 21**: Noticias internacionales
- **El Comercio**: Noticias de polÃ­tica
- **La RepÃºblica**: Noticias de sociedad/actualidad

## ğŸ¯ CategorÃ­as

- PolÃ­tica
- Internacional
- Sociedad / Actualidad
- TecnologÃ­a / Ciencia
- Cultura / Arte
- EconomÃ­a / Negocios
- OpiniÃ³n
- Estilo de vida / Tendencias
- Clima y Medio ambiente

## ğŸš€ InstalaciÃ³n

### Prerrequisitos

- Node.js 18+
- PostgreSQL
- npm o yarn

### Pasos

1. **Clonar el repositorio**
```bash
git clone https://github.com/Omar-Condori/Scraping.git
cd Scraping
```

2. **Instalar dependencias**
```bash
npm install
```

3. **Configurar variables de entorno**
```bash
cp env.example .env.local
```

Editar `.env.local`:
```env
DATABASE_URL="postgresql://usuario:password@localhost:5432/scraping_db"
```

4. **Configurar la base de datos**
```bash
# Generar cliente Prisma
npm run db:generate

# Aplicar migraciones
npm run db:push

# Inicializar categorÃ­as
npm run init:categories
```

5. **Ejecutar la aplicaciÃ³n**
```bash
npm run dev
```

La aplicaciÃ³n estarÃ¡ disponible en `http://localhost:3000`

## ğŸ“± Uso

### PÃ¡ginas Principales

- **Dashboard**: `http://localhost:3000` - Vista general y estadÃ­sticas
- **ArtÃ­culos**: `http://localhost:3000/articles` - Lista de noticias con filtros
- **AdministraciÃ³n**: `http://localhost:3000/admin` - Gestionar fuentes y categorÃ­as

### API Endpoints

- `GET /api/articles` - Obtener artÃ­culos
- `GET /api/categories` - Obtener categorÃ­as
- `GET /api/sources` - Obtener fuentes
- `GET /api/stats` - Obtener estadÃ­sticas
- `POST /api/scrape` - Ejecutar scraping manual
- `POST /api/scrape-dynamic` - Ejecutar scraping dinÃ¡mico

## ğŸ”§ Comandos Ãštiles

```bash
# Desarrollo
npm run dev

# ConstrucciÃ³n
npm run build
npm run start

# Base de datos
npm run db:generate
npm run db:push
npm run db:migrate
npm run db:studio

# Inicializar categorÃ­as
npm run init:categories
```

## ğŸ³ Docker

```bash
# Construir imagen
docker build -t scraping-app .

# Ejecutar con Docker Compose
docker-compose up -d
```

## ğŸ“ˆ CaracterÃ­sticas Avanzadas

- **Scraping DinÃ¡mico**: Agregar nuevas fuentes desde la interfaz
- **Filtros Inteligentes**: Buscar y filtrar por categorÃ­a, fuente, fecha
- **ImÃ¡genes y Descripciones**: ExtracciÃ³n automÃ¡tica de metadatos
- **PaginaciÃ³n**: NavegaciÃ³n eficiente de grandes volÃºmenes de datos
- **Responsive Design**: Optimizado para mÃ³viles y desktop

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

## ğŸ‘¨â€ğŸ’» Autor

**Omar Condori**
- GitHub: [@Omar-Condori](https://github.com/Omar-Condori)

---

## ğŸ“š Proyectos Anteriores

Este repositorio tambiÃ©n contiene scripts de Python para scraping de El Comercio:

- `scraping_elcomercio.py` - Scraping bÃ¡sico
- `scraping_elcomercio_contenido.py` - Con contenido completo
- `scraping_elcomercio_excel.py` - ExportaciÃ³n a Excel
- Y otros scripts especializados

Los archivos de Python estÃ¡n incluidos para referencia histÃ³rica del desarrollo del proyecto.